{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f992dd3-41f4-4fe2-a167-b73c332aa1fe",
   "metadata": {},
   "source": [
    "# Mini-Hackathon zu Sprachmodellen und Word Embeddings üßë‚Äçüíª\n",
    "\n",
    "Heute trainieren wir ein sog. *Sprachmodell*, also ein k√ºnstliches Modell nat√ºrlicher Sprache. Dazu \"f√ºttern\" wir einen darauf spezialisierten Algorithmus mit einer riesigen Menge an Sprachbeispielen. Als Sprachbeispiele verwenden wir einen Datensatz von [Wortschatz Leipzig](https://wortschatz.uni-leipzig.de/de) mit 100.000 S√§tzen aus deutschen Zeitungsartikeln aus dem Jahr 2022. \n",
    "\n",
    "Grob formuliert, schaut sich der Algorithmus die Beispiele immer und immer wieder an und analysiert, wie und wo die einzelnen W√∂rter darin auftreten. √úber die gro√üe Anzahl an Beispielen hinweg, findet der Algorithmus so Beziehungen zwischen den W√∂rtern sowie typische Muster, innerhalb derer sie auftreten (z.B. das vor \"bin\" typischerweise \"ich\" steht). Dadurch lernt der Algorithmus im Idealfall ann√§herungsweise die Bedeutung, die die W√∂rter f√ºr unser menschliches Sprachverst√§ndnis haben. \n",
    "\n",
    "Als Erstes installieren und importieren wir Python-Module, die wir zum Training ben√∂tigen und nehmen auch ein paar Einstellungen vor. F√ºhre diese Zelle aus, indem Du in sie reinklickst und anschlie√üend `Shift + Enter` dr√ºckt. Diesen Befehl sollst Du ab sofort bei jeder Code-Zelle ausf√ºhren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b92f6e-c292-49ee-aed5-c9373716595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import logging\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0fb21a-1dee-40ae-af31-3ad0662f27d7",
   "metadata": {},
   "source": [
    "## 1. Daten einlesen\n",
    "\n",
    "Nun wollen wir unseren Datensatz \"deu_news_2022_100K-sentences.txt\" aus dem Ordner \"data\" einlesen. Wenn Du die Datei mit dem Standardprogramm auf Deinem Rechner √∂ffnest, siehst Du, dass es sich um eine Art Tabelle handelt, mit je einer Zahl sowie einem vollst√§ndigen Satz pro Zeile. \n",
    "\n",
    "Der Algorithmus, den wir zum Trainieren unseres Sprachmodells verwenden, n√§mlich [*word2vec*](https://en.wikipedia.org/wiki/Word2vec), verlangt eine Liste mit S√§tzen, wobei jeder Satz wiederum als Liste mit W√∂rtern erwartet wird. Deshalb tokenisieren wir jeden Satz beim Einlesen und h√§ngen ihn als Wortliste der Satzliste an. Zus√§tzlich entfernen wir die Zahl am Zeilenanfang. F√ºhre die Zelle wiederum mittels `Shift + Enter` aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05540d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data/deu_news_2022_100K-sentences.txt\"\n",
    "\n",
    "with open(data, encoding=\"utf-8\") as f:\n",
    "    \n",
    "    #Tokenisieren inkl. Wegsplitten der Indizes am Zeilenanfang\n",
    "    sentences = [sentence.split()[1:] for sentence in tqdm(f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34080eb8",
   "metadata": {},
   "source": [
    "Inspizieren wir mal die eingelesenen Daten und lassen uns zwei S√§tze ausgeben. Eckige Klammern begrenzen bei Python Listen, wobei die einzelnen Elemente einer Liste jeweils mit Kommata voneinander abgetrennt sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7574fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[10:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73df177-046f-4d16-bdba-49f85502740f",
   "metadata": {},
   "source": [
    "Das sieht schon mal sehr gut aus!\n",
    "\n",
    "Allerdings sehen wir, dass die W√∂rter diverse Zeichen enthalten, die wir beim Training nicht gebrauchen k√∂nnen. Das Anf√ºhrungzeichen hinter 'Zustand\"' etwa hilft dem Algorithmus nicht, sich der Bedeutung von \"Zustand\" anzun√§hern. Ebenso unn√∂tig ist der \".\" am Ende von \"Klimafolgenforschung.\". Wir m√ºssen unsere Daten also bereinigen. Diesen Schritt nennt man *Preprocessing*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f5eee-1ff6-4970-964d-1ab8ea6ffeba",
   "metadata": {},
   "source": [
    "## 2. Daten bereinigen (Preprocessing)\n",
    "\n",
    "Die folgende Zelle definiert die Funktion `strip_special_signs`, die wir in der Zelle darunter auf unsere Daten anwenden. Sie identifiziert in einem ersten Schritt induktiv s√§mtliche nicht-alphanumerischen Zeichen in unseren Daten und entfernt diese im zweiten Schritt von s√§mtlichen Wortanf√§ngen und -enden. Den Code musst Du nicht im Detail nachvollziehen k√∂nnen. Wenn er Dich aber interessiert, kannst Du die Kommentare (alles was mit `#` beginnt) lesen. Sie beschreiben jeweils, was der Code davor bzw. darunter tut. F√ºhre die beiden Zellen in jedem Fall aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac60a54-06f4-4fdd-924f-59a5088baa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_special_signs(list_of_lists):\n",
    "    \n",
    "    #Schritt 1\n",
    "    special_signs = set() #Definieren eines noch leeren Sets (math. Menge), an das wir s√§mtliche Spezialzeichen (Zeichen, die nicht zu den normal_signs geh√∂ren), anh√§ngen\n",
    "    normal_signs = list(\"abcdefghijklmnopqrstuvwxyz√§√∂√º√ü1234567890\") #Schaffen einer Liste mit s√§mtlichen normalen alphanumerischen Zeichen\n",
    "    \n",
    "    print(\"Identifying special signs\") #Ausgabe des aktuellen Schritts\n",
    "    \n",
    "    #Iteration √ºber alle S√§tze...\n",
    "    for sentence in tqdm(list_of_lists):\n",
    "        #...und W√∂rter in unseren Daten\n",
    "        for word in sentence:\n",
    "            #√úberpr√ºfen, ob das erste Zeichen (mit Index 0) beim jeweiligen Wort in der Liste normal_signs ist und wenn NICHT...\n",
    "            if word[0].lower() not in normal_signs:\n",
    "                #...Anh√§ngen des jeweiligen Zeichens an das Set special_signs\n",
    "                special_signs.add(word[0])\n",
    "            #ebenfalls √úberpr√ºfen, ob letztes Zeichen beim jeweiligen Wort in der Liste normal_signs ist und wenn NICHT...\n",
    "            if word[-1].lower() not in normal_signs:  \n",
    "                #Anh√§ngen des jeweiligen Zeichens an das Set special_signs\n",
    "                special_signs.add(word[-1])\n",
    "        \n",
    "    #Schritt 2\n",
    "    print(\"Stripping off special signs\") #Ausgabe des aktuellen Schritts\n",
    "    \n",
    "    preprocessed_sentences = [] #Definieren einer noch leeren Liste, an die wir s√§mtliche bereingten S√§tze anh√§ngen\n",
    "    \n",
    "    #Iteration √ºber alle noch unbereinigten S√§tze...\n",
    "    for sentence in tqdm(list_of_lists):\n",
    "        preprocessed_sentence = [] #Definieren einer leeren Liste, an die wir s√§mtliche bereingten W√∂rter EINES Satzes anh√§ngen (Liste wird bei jeder Iteration neu geschaffen)\n",
    "        #...und unbereingten W√∂rter\n",
    "        for word in sentence:\n",
    "            #Entfernen (strip) aller Spezialzeichen am Wortanfang und -ende\n",
    "            preprocessed_word = word.strip(\"\".join(special_signs))\n",
    "            #Sofern Wort l√§nger als null Buchstaben...\n",
    "            if len(preprocessed_word) > 0:\n",
    "                #Anh√§ngen an Liste mit s√§mtlichen bereinigten W√∂rtern\n",
    "                preprocessed_sentence.append(preprocessed_word)\n",
    "        #Anh√§ngen der Liste mit bereinigten W√∂rtern an Liste mit bereinigten S√§tzen\n",
    "        preprocessed_sentences.append(preprocessed_sentence)\n",
    "        \n",
    "    #R√ºckgabe der Liste mit bereinigten S√§tzen\n",
    "    return preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639394f-b9b7-4246-afce-97e64c4534f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentences = strip_special_signs(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d14413-6e21-47ed-8fb4-f3c47a08ed6e",
   "metadata": {},
   "source": [
    "`preprocessed_sentences` bezeichnet jetzt die Liste mit bereinigten S√§tzen. Schauen wir uns nochmal die gleichen S√§tze wie oben an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b4331-44a6-4493-9038-8292b599e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_sentences[10:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde3fca-b543-446e-a73f-8169c7453c36",
   "metadata": {},
   "source": [
    "Das hat doch wunderbar geklappt. \n",
    "\n",
    "Nun sind unsere Daten bereit f√ºrs Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a4ecb",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609588d",
   "metadata": {
    "tags": []
   },
   "source": [
    "W√§hrend die Bedeutung von W√∂rtern bei Menschen im Sprachzentrum des Gehirns abgespeichert ist, so werden Wortbedeutungen bei word2vec in Form von Vektoren repr√§sentiert, also grob formuliert als Zahlenreihen, z.B. so: \n",
    "\n",
    "    [0.9823969, -0.16720027,  0.69778556, -0.10027876,  0.70647484,  1.0204794, ...].\n",
    "\n",
    "### 3.1. Was ist ein Vektor?\n",
    "\n",
    "Ein Vektor besteht aus einer bestimmten Anzahl an Zahlen, wobei jede Zahl angibt, inwiefern ein bestimmtes *Feature* bei einem gegebenen Wort zutrifft. Vereinfacht kann man sich einen Vektor als eine Reihe an numerischen Antworten auf sinnvolle Fragen vorstellen. Die erste Zahl im Vektor (das erste Feature) st√ºnde z.B. immer f√ºr die Anzahl an Buchstaben in einem gegebenen Wort, die zweite Zahl f√ºr die Auftretensh√§ufigkeit im Satz, etc. Ein komplexer Algorithmus wie word2vec kodiert allerdings keine f√ºr Menschen sinnvollen Features (Frage-Antwort-Paare), sondern die abstrakten Beziehungen und Muster zwischen W√∂rtern, die er beim Training entdeckt. Bei gen√ºgend Daten kann ein Modell so durchaus semantisch-syntaktisch sinnvolle Repr√§sentationen von W√∂rtern erlernen. Insgesamt entsteht beim Training ein Vektorraum, in dem die einzelnen Wortvektoren eingebettet sind, weswegen wir auch von *Word Embeddings* sprechen.\n",
    "\n",
    "### 3.2. Wie funktioniert das Training?\n",
    "\n",
    "Grunds√§tzlich unterscheidet man beim Training eines Modells zwischen √ºberwachtem und un√ºberwachtem Lernen (*supervised* vs. *unsupervised learning*). Bei √ºberwachtem Lernen wird ein Algorithmus mit annotierten Daten gef√ºttert, z.B. Bildern von Katzen und anderen Tieren, wobei jedes Bild mit dem Label \"Katze\" oder \"Nicht-Katze\" versehen ist. Ein Katzendetektor-Algorithmus hat beim Training Zugang zur \"Wahrheit\", also zur korrekten Antwort (\"Katze\" oder \"Nicht-Katze\"). Ziel des Trainings ist es, dass der Algorithmus √ºber die Trainingsdaten hinaus zu *generalisieren* lernt, d.h. dass er nach dem Training auch unannotierten Input korrekt als Katze oder Nicht-Katze bestimmen kann. \n",
    "\n",
    "Un√ºberwachtes Lernen hingegen ben√∂tigt keinerlei menschlichen Input, abgesehen von unannotierten Trainingsdaten. Word2vec ist ein un√ºberwachter Algorithmus. F√ºrs Training schafft sich word2vec ganz einfach seine eigenen Trainingsdaten: Basierend auf den vorangehenden und folgenden W√∂rtern innerhalb eines Satzes versucht word2vec ein verdecktes Wort in der Mitte vorherzusagen: Bei \"Der schwarze Hund _____ laut\" w√§re \"bellt\" z.B. eine gute Vorhersage. Die Idee hinter word2vec und Word Embeddings im Allgemeinen ist nun, dass der Algorithmus f√ºr √§hnliche W√∂rter √§hnliche Vektoren erlernt, denn √§hnliche W√∂rter kommen in √§hnlichen Kontexten (mit √§hnlichen vorangehenden und folgenden W√∂rtern) vor. \"knurrt\", das ebenfalls ein guter L√ºckenf√ºller w√§re, sollte demnach einen √§hnlichen Vektor wie \"bellt\" haben. Die √Ñhnlichkeit von W√∂rtern gem√§√ü unseres Sprachmodells schauen wir uns unten im Detail an. Zuerst m√ºssen wir das Modell nun aber trainieren.\n",
    "\n",
    "---\n",
    "\n",
    "Wir legen dazu ein paar Parameter fest, u.a.:\n",
    "\n",
    "- `vector_size`, das die Anzahl an Features pro Vektor festlegt.\n",
    "- `window` das festlegt, wie viele W√∂rter vor bzw. nach dem verdeckten Wort als Kontext beim Training ber√ºcksichtigt werden sollen.\n",
    "- `min_count`, das festlegt, wie oft ein Wort im Datensatz mindestens vorkommen muss, um beim Training ber√ºcksichtigt zu werden.\n",
    "- `epochs`, das festlegt, wie oft der Algorithmus den ganzen Datensatz analysieren soll.\n",
    "\n",
    "Und los geht's! Das Training nimmt ein paar Sekunden in Anspruch. Es ist fertig, wenn das Sternchen links neben der Zelle unten durch eine Zahl ersetzt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8310964",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(preprocessed_sentences, vector_size=200, window=6, min_count=3) \n",
    "model.train(preprocessed_sentences, total_examples=len(preprocessed_sentences), epochs=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97d83e",
   "metadata": {},
   "source": [
    "Nun ist das Modell fertig trainiert.\n",
    "\n",
    "Schauen wir uns mal den Vektor des Worts \"Universit√§t\" an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7623742-215b-44c2-935b-633169e8e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"Universit√§t\"\n",
    "print(model.wv[search_term]) #wv steht f√ºr word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798334db-07a4-4127-8d25-f8f6eceda05b",
   "metadata": {},
   "source": [
    "Spiel gerne mit anderen W√∂rter herum, indem Du sie bei `search_term` zwischen den Anf√ºhrungszeichen einsetzt. Bei W√∂rtern, die das Modell nicht kennt (weil sie nicht (gen√ºgend oft) in unseren Daten vorkamen) erh√§ltst Du einen `KeyError`.\n",
    "\n",
    "## 4. Das Modell\n",
    "\n",
    "Insgesamt sind diese Vektoren komplett nichtssagend f√ºr unser Sprachverst√§ndnis. Schauen wir aber, ob das Modell dennoch die Semantik von \"Universit√§t\" auf seine eigene, vektorielle Weise einfangen konnte. Dies k√∂nnen wir etwa tun, in dem wir uns das √§hnlichste Wort im Sprachmodell ausgeben lassen, also dasjenige Wort mit dem √§hnlichsten Vektor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e157a-fa2b-401e-a1ae-36c503aadff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"Universit√§t\"\n",
    "most_similar = pd.DataFrame(model.wv.most_similar(search_term, topn=10), columns=[\"Word\", \"Similarity\"], index=range(1,11))\n",
    "most_similar.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9c32f-bfd5-456b-a0bf-dbe986061a67",
   "metadata": {},
   "source": [
    "Vermutlich kommt \"Hochschule\" als √§hnlichstes Wort heraus. Faszinierend, oder? Setze gerne weitere W√∂rter bei `search_term` zwischen den Anf√ºhrungszeichen ein!\n",
    "\n",
    "‚ö†Ô∏è Achtung: Hier steht \"vermutlich\", da jedes Modell unterschiedlich ist, auch wenn die Trainingsdaten identisch waren. Dies liegt daran, dass unser Algorithmus nicht *deterministisch* ist. Ganz am Anfang des Trainings werden den Features der Vektoren n√§mlich zuf√§llige Werte zugewiesen. Das Training des Algorithmus besteht dann darin, die Features von Runde zu Runde anzupassen, um bessere Vorhersagen zu erzielen (also das verdeckte Wort besser zu erraten).\n",
    "\n",
    "Schauen wir uns die n√§chst√§hnlichen W√∂rter zu \"Universit√§t\" ebenfalls an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296daf15-84a0-4399-8c06-274d282892fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9547232-ae1b-42b0-bf4c-27332069c857",
   "metadata": {},
   "source": [
    "Sp√§testens hier sollten sich Unterschiede zwischen verschiedenen Modellen zeigen. Vermutlich sind auch nicht mehr alle W√∂rter f√ºr unser menschliches Sprachgef√ºhl √§hnlich zu \"Universit√§t\". \n",
    "\n",
    "Wir k√∂nnten nun mit den Parametern oben experimentieren (z.B. mehr Epochen oder ein gr√∂√üeres/kleineres Kontextfenster), um bessere Resultate zu erzielen. Zielf√ºhrender ist es jedoch, das Modell mit einem gr√∂√üeren Datensatz zu f√ºttern. Die Quantit√§t an Trainingsdaten ist absolut entscheidend f√ºr ein gutes Sprachmodell, wobei die Qualit√§t der Trainingsdaten (d.h. z.B., ob sie ausgewogen und repr√§sentativ sind) auch nicht au√üer Acht gelassen werden sollte! \n",
    "\n",
    "Da wir an der Qualit√§t kurzfristig nichts √§ndern k√∂nnen, verzehnfachen wir einfach mal die Quantit√§t. In der n√§chsten Code-Zelle lesen wir den Datensatz \"data/deu_news_2022_1M-sentences.txt\" ein, bereinigen ihn auf dieselbe Weise wie oben und trainieren ein neues, gr√∂√üeres Modell. Das Training mit 1 Million S√§tze dauert einige Minuten. Im Outputfenster siehst Du, in welcher Epoche der Algorithmus gerade steckt (sobald das Preprocessing beendet ist). Sollte Dein Computer √ºber wenig Rechenleistung verf√ºgen, kannst Du die folgende Code-Zelle auslassen (also nicht ausf√ºhren) und die weiteren Berechnungen basierend auf dem kleineren Modell durchf√ºhren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201dd1cf-a7b1-4004-a80c-bb8707b66590",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = \"data/deu_news_2022_1M-sentences.txt\"\n",
    "\n",
    "with open(data, encoding=\"utf-8\") as f:\n",
    "    sentences = [sentence.split()[1:] for sentence in tqdm(f)]\n",
    "    \n",
    "preprocessed_sentences = strip_special_signs(sentences)\n",
    "model = gensim.models.Word2Vec(preprocessed_sentences, vector_size=200, window=6, min_count=3) \n",
    "model.train(preprocessed_sentences, total_examples=len(preprocessed_sentences), epochs=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56bee6-1592-404f-a1e2-b1c8bfe6d6f6",
   "metadata": {},
   "source": [
    "Schauen wir uns nun die zehn √§hnlichsten W√∂rter zu \"Universit√§t\" an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd884364-dbe6-465f-b2e1-a9aa145a0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"Universit√§t\"\n",
    "most_similar = pd.DataFrame(model.wv.most_similar(search_term, topn=10), columns=[\"Word\", \"Similarity\"], index=range(1,11))\n",
    "most_similar.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a341e1f-fd38-41f6-9d3b-b0130b8c7107",
   "metadata": {},
   "source": [
    "Dies sieht schon viel besser aus! \n",
    "\n",
    "Wenn wir nicht blo√ü an den √§hnlichsten W√∂rtern zu einem bestimmten Wort interessiert sind, sondern daran, wie √§hnlich bestimmte Wortpaare zueinander sind, k√∂nnen wir folgenderma√üen vorgehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77835c17-9073-4cb0-8e19-6212a96778a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    (\"Auto\", \"Fahrzeug\"),   \n",
    "    (\"Auto\", \"Fahrrad\"),   \n",
    "    (\"Auto\", \"Flugzeug\"), \n",
    "    (\"Auto\", \"Haferflocken\"), \n",
    "    (\"Auto\", \"Zahnb√ºrste\")]\n",
    "\n",
    "for w1, w2 in pairs:\n",
    "    print(f\"{w1} und {w2:12} sind sich zu {model.wv.similarity(w1, w2)*100:.2f}% √§hnlich.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807b1c4-abcf-4f5c-a11c-08188c1384a0",
   "metadata": {},
   "source": [
    "Auch interessant ist es, herauszufinden, welches Wort am wenigsten zu anderen gegebenen W√∂rtern passt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da4eab-522d-494c-b07f-95c0e856b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.doesnt_match([\"Berlin\", \"Hamburg\", \"Z√ºrich\", \"Dresden\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db20e3-85be-48a4-8bc9-90967926487d",
   "metadata": {},
   "source": [
    "Mit am faszinierendsten ist sog. *Vektoraddition* bzw. *-subtraktion*: Wenn wir vom Vektor f√ºr das Wort \"Paris\" den Vektor f√ºr das Wort \"Deutschland\" addieren, anschlie√üend aber den Vektor f√ºr das Wort \"Frankreich\" subtrahieren, was k√∂nnte dann daraus resultieren...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09906c96-a2ee-4d9f-b283-bd8098eb0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['Paris', 'Deutschland'], negative=['Frankreich'], topn=1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13a3a6-9002-4256-b1a5-48d767b43985",
   "metadata": {},
   "source": [
    "Das Modell hat also gelernt, dass sich Berlin zu Deutschland verh√§lt wie sich Paris zu Frankreich verh√§lt. \n",
    "\n",
    "Und wie sieht es mit Pr√§sidenten aus? Was Biden f√ºr die USA ist, ist ... f√ºr Russland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e03bdb-d6e2-40da-8979-7cec9888eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['Biden', 'Russland'], negative=['USA'], topn=1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45682074-bbf3-4c8c-82e1-b7b928800d04",
   "metadata": {},
   "source": [
    "...oder was kommt dabei raus, wenn wir vom Vektor f√ºr \"geht\" den Vektor f√ºr \"gestern\" addieren, denjenigen f√ºr \"heute\" jedoch subtrahieren?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff8714-9c0b-4456-9905-c461848b72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['geht', 'gestern'], negative=['heute'], topn=1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ecf5a8-f633-4a6b-bca1-95600049d21a",
   "metadata": {},
   "source": [
    "Spannend, oder?\n",
    "\n",
    "Experimentiere bei s√§mtlichen \"√Ñhnlichkeitsmethoden\" mit eigenen Begriffen herum, um herauszufinden, wie gut sich der Algorithmus Deinem Sprachverst√§ndnis nach Wortbedeutungen aneignen konnte.\n",
    "\n",
    "Wir k√∂nnen uns den Vektorraum auch visualisieren lassen, zwar nicht in all seinen 200 Dimensionen (jedes Feature entspricht einer Koordinate in einer Dimension), da das f√ºr Menschen schlicht nicht vorstellbar ist, aber reduziert auf drei Dimensionen. Wir benutzen dazu den Embedding Projector von TensorFlow. [Hier](https://projector.tensorflow.org/?config=https://raw.githubusercontent.com/yannickfrommherz/Netzwerktreffen-virTUos/a4e3bf89bf3f4e6d8a0fc434ecfaa4f34997e16f/config.json) findest Du eine Visualisierung des mit einer Million S√§tze trainierten Modells. Das Laden des Vektorraums kann ein bisschen dauern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "test-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
